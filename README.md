In collaboration with IEEE's data science division, our team at the University of Pennsylvania embarked on a mission to create an advanced web scraping tool this semester. Our project aimed to develop 'scholarscraper', a specialized Python library designed to navigate the internet in search of web pages relevant to specific academic authors. Such pages might include academic profiles like those on Google Scholar or LinkedIn, articles highlighting the author's research achievements, or even GitHub repositories linked to their work.
The impetus for our project came from the need to efficiently sift through the vast expanse of the internet to find "scholarly entities" â€” essentially, any online content that directly contributes to understanding an author's academic and professional footprint. Our method involved starting with "seed information" obtained from IEEE's comprehensive data lake. This information included basics like the author's name, their latest academic affiliation, and a list of their published works.
Our approach to tackling this challenge was two-pronged, employing both Top-Down and Bottom-Up strategies. The Top-Down method made use of the Common Crawl archive, a massive collection of web data that has been accumulated since 2008. On the other hand, the Bottom-Up strategy utilized Google's search capabilities to execute focused searches using the seed information as a guide.
The 'scholarscraper' API we developed stands as a testament to our team's innovative use of machine learning (ML) and natural language processing (NLP) technologies. These technologies were instrumental in identifying, categorizing, and sorting the data we discovered across the web into predefined buckets of interest.
The work we did is now owned by IEEE, so we can't share it with everyone. However, we hope that our presentation and report will show the hard work and new ideas we brought to this project.





